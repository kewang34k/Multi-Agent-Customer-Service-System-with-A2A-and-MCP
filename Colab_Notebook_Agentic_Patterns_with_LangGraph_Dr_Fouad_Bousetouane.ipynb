{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K8Z457mXRJ1D"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Colab Notebook: Agentic Patterns with LangGraph"
      ],
      "metadata": {
        "id": "Hq8cZUZuRU0L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install and Import"
      ],
      "metadata": {
        "id": "zU4XUHvvRYWr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required libraries\n",
        "!pip install -q -U langgraph langchain-openai langchain-core"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LUNAvEk3RWMN",
        "outputId": "16f067da-73ab-4675-f691-7da34394b035"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/156.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m156.8/156.8 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/81.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m81.9/81.9 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/469.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m469.9/469.9 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.2/46.2 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m208.3/208.3 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain 0.3.27 requires langchain-core<1.0.0,>=0.3.72, but you have langchain-core 1.0.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reactive Agent (Single-Step Response)"
      ],
      "metadata": {
        "id": "KXFPAq7WSYal"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "LANGRAPH AGENT: MATH CALCULATOR\n",
        "================================\n",
        "\n",
        "A simple math tutor agent that solves arithmetic problems in 2 steps:\n",
        "\n",
        "STEP 1 - LLM NODE:\n",
        "   - Receives user's math question\n",
        "   - Extracts the numeric expression (e.g., \"8 * (5 + 2)\")\n",
        "   - Uses agent role to guide extraction\n",
        "\n",
        "STEP 2 - CALCULATOR NODE:\n",
        "   - Takes the expression from LLM\n",
        "   - Evaluates it using Python's calculator tool\n",
        "   - Returns the final result\n",
        "\n",
        "FLOW: User Question â†’ LLM (extract expression) â†’ Calculator (compute) â†’ Result\n",
        "\n",
        "Components:\n",
        "- AgentState: Stores input question and output result\n",
        "- AGENT_ROLE: Defines the math tutor behavior\n",
        "- calculator tool: Performs arithmetic operations\n",
        "- StateGraph: Connects LLM and Calculator in sequence\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "cSP6ruJ__GZn",
        "outputId": "75d97111-5b95-41e3-d5f8-b826b0335d3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nLANGRAPH AGENT: MATH CALCULATOR\\n================================\\n\\nA simple math tutor agent that solves arithmetic problems in 2 steps:\\n\\nSTEP 1 - LLM NODE:\\n   - Receives user\\'s math question\\n   - Extracts the numeric expression (e.g., \"8 * (5 + 2)\")\\n   - Uses agent role to guide extraction\\n\\nSTEP 2 - CALCULATOR NODE:\\n   - Takes the expression from LLM\\n   - Evaluates it using Python\\'s calculator tool\\n   - Returns the final result\\n\\nFLOW: User Question â†’ LLM (extract expression) â†’ Calculator (compute) â†’ Result\\n\\nComponents:\\n- AgentState: Stores input question and output result\\n- AGENT_ROLE: Defines the math tutor behavior\\n- calculator tool: Performs arithmetic operations\\n- StateGraph: Connects LLM and Calculator in sequence\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  Imports\n",
        "from langgraph.graph import StateGraph, END, START\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.tools import Tool\n",
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
        "\n"
      ],
      "metadata": {
        "id": "vy6iKfD-SfPH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import StateGraph, START, END\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.tools import tool\n",
        "\n",
        "# --- Define schema ---\n",
        "class AgentState(dict):\n",
        "    input: str\n",
        "    output: str\n",
        "\n",
        "AGENT_ROLE = \"\"\"You are a math tutor assistant.\n",
        "Your job is to extract mathematical expressions and help solve them.\"\"\"\n",
        "\n",
        "# --- Define a tool ---\n",
        "@tool\n",
        "def calculator(expression: str) -> str:\n",
        "    \"\"\"Performs basic arithmetic calculations.\"\"\"\n",
        "    try:\n",
        "        return str(eval(expression))\n",
        "    except Exception as e:\n",
        "        return f\"Error: {e}\"\n",
        "\n",
        "# --- Initialize LLM ---\n",
        "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
        "\n",
        "# --- LLM node ---\n",
        "def llm_node(state: AgentState):\n",
        "    \"\"\"LLM decides the expression to calculate.\"\"\"\n",
        "    question = state[\"input\"]\n",
        "    # Use the agent role to guide the LLM's behavior\n",
        "    reasoning = llm.invoke([\n",
        "        {\"role\": \"system\", \"content\": AGENT_ROLE},\n",
        "        {\"role\": \"user\", \"content\": f\"Extract ONLY the mathematical expression from: {question}\\nReturn ONLY the expression with numbers and operators, nothing else. No words, no explanations.\"}\n",
        "    ])\n",
        "    # Clean the output - remove any extra text or quotes\n",
        "    expression = reasoning.content.strip().strip('\"').strip(\"'\")\n",
        "    print(f\"Extracted expression: {expression}\")\n",
        "    return {\"output\": expression}\n",
        "\n",
        "# --- Tool node ---\n",
        "def tool_node(state: AgentState):\n",
        "    \"\"\"Tool executes the computation requested by the LLM.\"\"\"\n",
        "    result = calculator.invoke({\"expression\": state[\"output\"]})\n",
        "    return {\"output\": f\"The result is {result}\"}\n",
        "\n",
        "# ---  Build the Agent Graph ---\n",
        "# LangGraph lets us connect different components (nodes) together\n",
        "# into a directed flow â€” like a neural network for decision and action.\n",
        "\n",
        "graph = StateGraph(AgentState)  # Initialize the graph with the state structure (input/output fields)\n",
        "\n",
        "# Add each node in the workflow\n",
        "graph.add_node(\"LLM\", llm_node)           #  Node 1: The LLM â€” interprets the question, extracts the math\n",
        "graph.add_node(\"Calculator\", tool_node)   #  Node 2: The Tool â€” performs the actual computation\n",
        "\n",
        "# Define the flow (edges) between nodes\n",
        "graph.add_edge(START, \"LLM\")              # Start â†’ LLM (entry point)\n",
        "graph.add_edge(\"LLM\", \"Calculator\")       # LLM output is sent to the Calculator tool\n",
        "graph.add_edge(\"Calculator\", END)         # End after the calculator returns a result\n",
        "\n",
        "# --- Compile and run ---\n",
        "agent = graph.compile()\n",
        "result = agent.invoke({\"input\": \"What is 8 * (5 + 2)?\"})\n",
        "print(result[\"output\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ItRz0jVYVqgw",
        "outputId": "69c7a3c5-2091-42c1-b97f-af9fb68fa97a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted expression: 8 * (5 + 2)\n",
            "The result is 56\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hVP5qDiyacNF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ReAct Agent â€” Web Search + Summarization"
      ],
      "metadata": {
        "id": "hc6Ca7VEk2kZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install duckduckgo-search"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IU1UTvQ1k3dv",
        "outputId": "704667a1-3ef2-4f31-8069-ee24be79286f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting duckduckgo-search\n",
            "  Downloading duckduckgo_search-8.1.1-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.12/dist-packages (from duckduckgo-search) (8.3.0)\n",
            "Collecting primp>=0.15.0 (from duckduckgo-search)\n",
            "  Downloading primp-0.15.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: lxml>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from duckduckgo-search) (5.4.0)\n",
            "Downloading duckduckgo_search-8.1.1-py3-none-any.whl (18 kB)\n",
            "Downloading primp-0.15.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: primp, duckduckgo-search\n",
            "Successfully installed duckduckgo-search-8.1.1 primp-0.15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "LANGRAPH AGENT: SEARCH ASSISTANT\n",
        "=================================\n",
        "\n",
        "This agent demonstrates a simple ReAct-style workflow with 3 phases:\n",
        "\n",
        "STEP 1 - REASONING (ğŸ§ ):\n",
        "   - Takes user input\n",
        "   - Uses LLM to extract clean search keywords\n",
        "   - Transitions to search phase\n",
        "\n",
        "STEP 2 - SEARCHING (ğŸ”):\n",
        "   - Takes the keywords from step 1\n",
        "   - Generates a Google search URL\n",
        "   - Transitions to reflection phase\n",
        "\n",
        "STEP 3 - REFLECTING (ğŸª):\n",
        "   - Uses LLM to summarize what the user will find\n",
        "   - Returns final answer\n",
        "   - Agent completes\n",
        "\n",
        "FLOW: User Input â†’ Reasoning â†’ Search â†’ Reflection â†’ Final Answer\n",
        "\n",
        "The agent uses:\n",
        "- StateGraph: Manages workflow between nodes\n",
        "- Conditional edges: Routes based on phase (reason â†’ search â†’ reflect â†’ done)\n",
        "- LLM calls: For reasoning and reflection\n",
        "- Tool: For generating search URLs\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "r8RUFrw8k6yu",
        "outputId": "e5436981-6275-40ac-e958-b9019687917c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nLANGRAPH AGENT: SEARCH ASSISTANT\\n=================================\\n\\nThis agent demonstrates a simple ReAct-style workflow with 3 phases:\\n\\nSTEP 1 - REASONING (ğŸ§ ):\\n   - Takes user input\\n   - Uses LLM to extract clean search keywords\\n   - Transitions to search phase\\n\\nSTEP 2 - SEARCHING (ğŸ”):\\n   - Takes the keywords from step 1\\n   - Generates a Google search URL\\n   - Transitions to reflection phase\\n\\nSTEP 3 - REFLECTING (ğŸª):\\n   - Uses LLM to summarize what the user will find\\n   - Returns final answer\\n   - Agent completes\\n\\nFLOW: User Input â†’ Reasoning â†’ Search â†’ Reflection â†’ Final Answer\\n\\nThe agent uses:\\n- StateGraph: Manages workflow between nodes\\n- Conditional edges: Routes based on phase (reason â†’ search â†’ reflect â†’ done)\\n- LLM calls: For reasoning and reflection\\n- Tool: For generating search URLs\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import StateGraph, START, END\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.tools import tool\n",
        "\n",
        "@tool\n",
        "def google_search(query: str) -> str:\n",
        "    \"\"\"Generate a Google search URL.\"\"\"\n",
        "    return f\"https://www.google.com/search?q={query.replace(' ', '+')}\"\n",
        "\n",
        "class AgentState(dict):\n",
        "    input: str\n",
        "    query: str\n",
        "    url: str\n",
        "    output: str\n",
        "    phase: str\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
        "\n",
        "AGENT_ROLE = \"\"\"You are a research assistant that helps users find information.\n",
        "Your job is to:\n",
        "1. Understand the user's question\n",
        "2. Generate a search query\n",
        "3. Provide a helpful summary of what they'll find\"\"\"\n",
        "\n",
        "def llm_node(state: AgentState):\n",
        "    phase = state.get(\"phase\", \"reason\")\n",
        "\n",
        "    if phase == \"reason\":\n",
        "        print(\"\\nğŸ§  PHASE 1: REASONING\")\n",
        "        print(f\"   Input: {state['input']}\")\n",
        "        response = llm.invoke([\n",
        "            {\"role\": \"system\", \"content\": AGENT_ROLE},\n",
        "            {\"role\": \"user\", \"content\": f\"Extract only the search keywords for: {state['input']}\\nReturn ONLY the keywords, nothing else.\"}\n",
        "        ])\n",
        "        query = response.content.strip().strip('\"')\n",
        "        print(f\"   Query: {query}\")\n",
        "        return {**state, \"query\": query, \"phase\": \"search\"}\n",
        "\n",
        "    if phase == \"reflect\":\n",
        "        print(\"\\nğŸª PHASE 3: REFLECTING\")\n",
        "        print(f\"   Analyzing results from search\")\n",
        "        summary = llm.invoke([\n",
        "            {\"role\": \"system\", \"content\": AGENT_ROLE},\n",
        "            {\"role\": \"user\", \"content\": f\"The user searched for: '{state['input']}'\\nProvide a 2-3 sentence summary of what information they would find about this topic.\"}\n",
        "        ])\n",
        "        print(f\"   Summary generated âœ“\")\n",
        "        return {**state, \"output\": summary.content, \"phase\": \"done\"}\n",
        "\n",
        "    return state\n",
        "\n",
        "def search_node(state: AgentState):\n",
        "    print(\"\\nğŸ” PHASE 2: SEARCHING\")\n",
        "    url = google_search.invoke({\"query\": state[\"query\"]})\n",
        "    print(f\"   URL: {url}\")\n",
        "    return {**state, \"url\": url, \"phase\": \"reflect\"}\n",
        "    # Keep all existing state values unchanged, but update \"url\" with new search URL\n",
        "    # and change \"phase\" to \"reflect\" (next step after search completes)\n",
        "\n",
        "# Build graph\n",
        "graph = StateGraph(AgentState)  # Initialize graph with our state schema\n",
        "graph.add_node(\"LLM\", llm_node)  # Add node for reasoning and reflection\n",
        "graph.add_node(\"Search\", search_node)  # Add node for search action\n",
        "\n",
        "# Define edges (connections between nodes)\n",
        "graph.add_edge(START, \"LLM\")  # Entry point: always start at LLM node\n",
        "graph.add_edge(\"Search\", \"LLM\")  # After search, always return to LLM for reflection\n",
        "\n",
        "# Conditional routing from LLM node based on phase\n",
        "graph.add_conditional_edges(\n",
        "    \"LLM\",  # From LLM node\n",
        "    lambda s: \"END\" if s.get(\"phase\") == \"done\" else \"Search\",  # Decision function: if done â†’ END, else â†’ Search\n",
        "    {\"END\": END, \"Search\": \"Search\"}  # Map decisions to actual destinations\n",
        "\n",
        ")\n",
        "# Run\n",
        "agent = graph.compile()\n",
        "result = agent.invoke({\"input\": \"latest trends in generative AI 2025\"})\n",
        "\n",
        "print(\"\\n\" + \"=\")\n",
        "print(\"FINAL RESULT\")\n",
        "print(\"=\")\n",
        "print(f\"ANSWER: {result['output']}\")\n",
        "print(f\"URL: {result['url']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yb8lUbjrlLlR",
        "outputId": "2ef7bc48-9df0-4f11-d3cd-03265332d6ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ§  PHASE 1: REASONING\n",
            "   Input: latest trends in generative AI 2025\n",
            "   Query: latest trends generative AI 2025\n",
            "\n",
            "ğŸ” PHASE 2: SEARCHING\n",
            "   URL: https://www.google.com/search?q=latest+trends+generative+AI+2025\n",
            "\n",
            "ğŸª PHASE 3: REFLECTING\n",
            "   Analyzing results from search\n",
            "   Summary generated âœ“\n",
            "\n",
            "=\n",
            "FINAL RESULT\n",
            "=\n",
            "ANSWER: By 2025, the latest trends in generative AI are expected to include advancements in multimodal AI systems that can seamlessly integrate text, image, and audio generation. There will likely be a focus on improving the ethical and responsible use of AI, with enhanced transparency and bias mitigation techniques. Additionally, generative AI is anticipated to play a significant role in personalized content creation, virtual reality, and the development of more sophisticated AI-driven tools for creative industries.\n",
            "URL: https://www.google.com/search?q=latest+trends+generative+AI+2025\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MH43gX4J2NzW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}